{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c00bf530",
      "metadata": {
        "id": "c00bf530"
      },
      "source": [
        "# Project 2: Classification\n",
        "\n",
        "This project asks you to perform various experiments with classification. The dataset we are using is a toy dataset for credit card fraud detection:\n",
        "\n",
        "https://www.kaggle.com/datasets/shubhamjoshi2130of/abstract-data-set-for-credit-card-fraud-detection\n",
        "\n",
        "You will write code and discussion texts into code and text cells in this notebook. \n",
        "\n",
        "If a block starts with TODO:, this means that you need to write something there. \n",
        "\n",
        "Some code had been written for you to guide the project. Don't change the already written code.\n",
        "\n",
        "## Grading\n",
        "The points add up to 40, that is 30 + 10 bonus points. While there is no difference between the regular and the bonus points, I recommend that you solve the problems labeled \"BONUS\" after you finished the other ones. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a440a191",
      "metadata": {
        "id": "a440a191"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import sklearn as sk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "691d1a9f",
      "metadata": {
        "id": "691d1a9f"
      },
      "source": [
        "## Setup for the project\n",
        "\n",
        "Here we load the dataset, and create the training and test datasets as numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "902ccefe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "902ccefe",
        "outputId": "125da236-b65c-4709-8a15-83e401b8af30"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3805bd02f801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"creditcard.csv\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtrue_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"N\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of rows {len(df.index)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The columns of the database {df.columns}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"isFradulent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'creditcard.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"creditcard.csv\",  true_values=[\"Y\"], false_values=[\"N\"])\n",
        "pd.show_versions()\n",
        "print(f\"Number of rows {len(df.index)}\")\n",
        "print(f\"The columns of the database {df.columns}\")\n",
        "df.value_counts(\"isFradulent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54bf2ec2",
      "metadata": {
        "id": "54bf2ec2"
      },
      "outputs": [],
      "source": [
        "xfields = [\n",
        "    'Average Amount/transaction/day',\n",
        "       'Transaction_amount', 'Is declined', 'Total Number of declines/day',\n",
        "       'isForeignTransaction', 'isHighRiskCountry', 'Daily_chargeback_avg_amt',\n",
        "       '6_month_avg_chbk_amt', '6-month_chbk_freq']\n",
        "\n",
        "df_shuffled = df.sample(frac=1) # shuffle the rows\n",
        "x = df_shuffled[xfields].to_numpy(dtype=np.float64)\n",
        "y = df_shuffled[\"isFradulent\"].to_numpy(dtype=np.float64)\n",
        "# the training data is the first 2000 rows, after shuffled\n",
        "training_data_x = x[:2000]\n",
        "training_data_y = y[:2000]\n",
        "# the test data is the remaining\n",
        "test_data_x = x[2000:]\n",
        "test_data_y = y[2000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "959fcb7c",
      "metadata": {
        "id": "959fcb7c"
      },
      "outputs": [],
      "source": [
        "print(\"Run this to help you with what number goes with what field:\")\n",
        "for i, x in enumerate(xfields):\n",
        "    print(f\"{i} = {x}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data experiments\n",
        "#training data x setup like rows     column 0, row 0-8 (avg amount, transaction, etc), training data y would be the fraudulent or not\n",
        "print(training_data_x[1999][0])\n",
        "print(training_data_x[398][5])\n",
        "\n",
        "\n",
        "print(training_data_x[31][5])\n",
        "print(training_data_y[33])\n"
      ],
      "metadata": {
        "id": "JNbQCjsfkuuV"
      },
      "id": "JNbQCjsfkuuV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ba69479d",
      "metadata": {
        "id": "ba69479d"
      },
      "source": [
        "## P1: Create an accuracy metric (7 pts)\n",
        "Create a simple accuracy metric function which for a pair of ground truth values $y$ and estimates $\\hat{y}$ (both of them arrays) calculates the accuracy of the estimate $\\hat{y}$. For instance, if you pass y = [1, 0, 1] and \n",
        "yhat = [1, 1, 0], the loss function should return 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bd1d0ca",
      "metadata": {
        "id": "9bd1d0ca"
      },
      "outputs": [],
      "source": [
        "def accuracy(y, yhat):\n",
        "    ## implement here\n",
        "    y_accuracy = np.array(y)\n",
        "    yhat_accuracy = np.array(yhat)\n",
        "\n",
        "    size = y_accuracy.size\n",
        "\n",
        "    compare = len(np.where(y_accuracy == yhat_accuracy)[0])\n",
        "\n",
        "    result = compare/size\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a577d090",
      "metadata": {
        "id": "a577d090"
      },
      "outputs": [],
      "source": [
        "# test your function here\n",
        "acc = accuracy([1, 0, 1], [1, 1, 0])\n",
        "print(f\"Accuracy is {acc}\") # should print 0.33..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "564999e3",
      "metadata": {
        "id": "564999e3"
      },
      "source": [
        "## P2: Implement a majority classifier (7 pts)\n",
        "This classifier will always return the most likely value. Training the classifier means determining what is the most likely value (regardless vhat value you pass to it). For instance, if more than half of the transactions are fraudulent, then you just return fraudulent always. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7207afba",
      "metadata": {
        "id": "7207afba"
      },
      "outputs": [],
      "source": [
        "def classify_majority(x, theta):\n",
        "    # whatever the value of x, we return the theta\n",
        "    theta = theta\n",
        "    return theta\n",
        "\n",
        "# TODO: implement the train majority function\n",
        "def train_majority(training_x, training_y):\n",
        "    # this function will have to determine which is more likely to \n",
        "    # be the value of y, one (true) or zero (false)\n",
        "\n",
        "    #convert floats from training y to integers\n",
        "    int_array = np.rint(training_y).astype(int) \n",
        "\n",
        "    #finds tha majority value of y \n",
        "    majority = np.bincount(int_array).argmax()\n",
        "\n",
        "\n",
        "    # C.N 1: if x equals a certain average value that produces the y value, make the threshhold. anything before is true, anything before is false \n",
        "    return majority"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61379a5",
      "metadata": {
        "id": "d61379a5"
      },
      "outputs": [],
      "source": [
        "# TODO: use the train_majority function to find the theta value for the training dataset\n",
        "theta = 0\n",
        "theta = train_majority(training_data_x, training_data_y)\n",
        "print(theta)\n",
        "# TODO: now use the theta value to create the test_data_yhat array which contains the classification for each test value \n",
        "test_data_yhat = np.full_like(test_data_y, theta)\n",
        "# TODO: now calculate the accuracy of the classifier using the function implemented in P1, and print it out\n",
        "print(accuracy(test_data_y, test_data_yhat))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc53d27d",
      "metadata": {
        "id": "dc53d27d"
      },
      "source": [
        "TODO: Discuss here the performance of the majority classifier. Would this beat a classifier that just returns random values? "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No it would not due to how the random values have a chance to not be accurate at all. At least with the majority classifier it'll be semi accurcate due to its basis on factual data.\n"
      ],
      "metadata": {
        "id": "wEGWOASOIRVU"
      },
      "id": "wEGWOASOIRVU"
    },
    {
      "cell_type": "markdown",
      "id": "1c63bf75",
      "metadata": {
        "id": "1c63bf75"
      },
      "source": [
        "## P3: Implement a hand engineered classifier (8 pts)\n",
        "\n",
        "Engineer by hand a classifier function that predicts whether  a transaction is  fraudulent or not. Your function should have a $\\theta$ parameter which allows you to tweak it. \n",
        "The problem requires you to design a function that performs this classification, tweak its parameters, and measure its accuracy for the best parametrization you found. You should aim for a function that, at minimum, performs better than the majority classifier. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66d0d477",
      "metadata": {
        "id": "66d0d477"
      },
      "outputs": [],
      "source": [
        "# TODO: implement here your hand-engineered classifier\n",
        "# The example below is just a very bad example, but it gives you an idea of how you can reason about the classification problem.\n",
        "# In your implementation, you should try to actually find some kind of clever algorithm. You can also use more complex parametrizations\n",
        "\n",
        "def classify_handwritten(x, theta):\n",
        "    \"\"\"Fraudulent transaction classifier. In this test example, we classify every foreign transaction as fraudulent. Transactions larger than theta[0] are also fraudulent\"\"\"\n",
        "    if x[5] == 1:\n",
        "        return 1\n",
        "    else:\n",
        "        if x[4] == 1:\n",
        "            return 1\n",
        "    if x[1] > theta[0]:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def average_finder(training_data_x, training_data_y):\n",
        "    #obtain all x's that is flagged as fraudluent\n",
        "    #go through data and if training data y is marked as fraudulent, record its index of training data x add it to a np array\n",
        "    af_y_array = np.rint(training_data_y).astype(int) \n",
        "    fradulentcounter = 0\n",
        "    total = 0\n",
        "    j = 0\n",
        "\n",
        "    for i in af_y_array:\n",
        "      if i == 1:\n",
        "        #if training data is fraudulent, insert that row's average transaction into the average array so we can calculate the average\n",
        "        fradulentcounter = fradulentcounter + 1\n",
        "        total = total + training_data_x[j][0]\n",
        "        j+=1\n",
        "    # get the average value of the average transactions value\n",
        "    average = total / fradulentcounter\n",
        "    return average\n",
        "\n",
        "\n",
        "def banaag_classifier(test_data_x, average, j):\n",
        "    #if test data x average meets threshold or above, mark it is a 1, else 0\n",
        "    comparer_bc = test_data_x[j][0]\n",
        "\n",
        "    #print(\"Comparer_BC:\", comparer_bc)\n",
        "    #print(\"Average:\",average)\n",
        "    if comparer_bc >= average+490:\n",
        "\n",
        "      #print(\"Result: 1\")\n",
        "      return 1\n",
        "    if test_data_x[j][5] == 1:\n",
        "      return 1\n",
        "    else:\n",
        "      #print(\"Result: 0\")\n",
        "\n",
        "      return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a723623d",
      "metadata": {
        "id": "a723623d"
      },
      "outputs": [],
      "source": [
        "#TODO: Now, run some experiments with your function. Experiment with different values of the parameter theta.\n",
        "averageP3 = average_finder(training_data_x, training_data_y)\n",
        "y_hat_banaag_classifier = np.zeros(0)\n",
        "j = 0\n",
        "\n",
        "# call banaag_classifier for each value of test data x, return each result to declared array (for loop)\n",
        "for i in test_data_x:\n",
        "  comparer = banaag_classifier(test_data_x, averageP3, j)\n",
        "  #print(comparer)\n",
        "  \n",
        "  if comparer == 1:\n",
        "    y_hat_banaag_classifier = np.insert(y_hat_banaag_classifier, j, 1) #insert are problem, insert at index j?\n",
        "  else:\n",
        "    y_hat_banaag_classifier = np.insert(y_hat_banaag_classifier, j, 0)\n",
        "  j+=1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "G_bPjf5slGMa"
      },
      "id": "G_bPjf5slGMa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86997918",
      "metadata": {
        "id": "86997918",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570eac59-9fdc-4790-be2d-c3333d1c9346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9209302325581395\n"
          ]
        }
      ],
      "source": [
        "# TODO: calculate the accuracy of the classifier on the test data with the best\n",
        "# theta found above and print it.\n",
        "print(accuracy(test_data_y, y_hat_banaag_classifier))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c319f058",
      "metadata": {
        "id": "c319f058"
      },
      "source": [
        "TODO: Describe in one paragraph your experiments and evaluation. Discuss the overall accuracy your classifier. Did you manage to beat the \"majority\" classifier? Comment on how easy or hard was to do this. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  When it came to my experimentation, I ran into the most issues with the synthax of Numpy. Overall, I got the average of the transactions for training_data_x and compared it to each value of test_data_x. If it was greater than by the threshhold I tuned, than it was marked as fraud. The next checker was to see if it was in a high risk country. If it was, I marked it as fraud. Overall, the accuracy increased by six percent, which is pretty solid but can be improved upon. Like I have previously stated, it was quite hard adapting to the python synthax and that was my hardest challenge.\n"
      ],
      "metadata": {
        "id": "ZJuyMZv_ns21"
      },
      "id": "ZJuyMZv_ns21"
    },
    {
      "cell_type": "markdown",
      "id": "e0548dea",
      "metadata": {
        "id": "e0548dea"
      },
      "source": [
        "## P4: Implement a logistic regression classifier using sklearn (8 pts)\n",
        "Implement a logistic regression function using the sklearn library. \n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b91956b",
      "metadata": {
        "id": "1b91956b"
      },
      "outputs": [],
      "source": [
        "# TODO: implement the logistic regression here in a function \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def logisticregression(training_data_x, training_data_y, test_data_x, test_data_y):\n",
        "  model = LogisticRegression().fit(training_data_x, training_data_y)\n",
        "\n",
        "  score = model.score(test_data_x, test_data_y)\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7af53924",
      "metadata": {
        "id": "7af53924",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "655a5c6e-c90c-4e7a-ae90-529e73091c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.987906976744186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ],
      "source": [
        "# TODO: now, run some experiments with it, and measure the accuracy with various parametrizations. In particular, you should run it with and without regularization. \n",
        "# In the last line, print the accuracy with the best parameters.\n",
        "print(logisticregression(training_data_x, training_data_y, test_data_x, test_data_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20b3bfac",
      "metadata": {
        "id": "20b3bfac"
      },
      "source": [
        "\n",
        "TODO: Describe in one paragraph your experiments and evaluation of the Logistic Regression classifier. Consider things such as accuracy, training time, ease of tweaking of the parameters. Compare it with the accuracy of the hand-engineered classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression classifier is much easier to impliment and train compared to my hand engineered classifier. When it comes to tweaking the parameters, I only have to limit what data the function gets access too, and tweak which indexes are chosen. Compared to the hand written classifier, this is much easier."
      ],
      "metadata": {
        "id": "QXKCHq0bEU5f"
      },
      "id": "QXKCHq0bEU5f"
    },
    {
      "cell_type": "markdown",
      "id": "a5fcd1c6",
      "metadata": {
        "id": "a5fcd1c6"
      },
      "source": [
        "## P5 Bonus: Implement a random forest classifier using sklearn (5 pts)\n",
        "Implement a random forest classifier using sklearn \n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9663e31d",
      "metadata": {
        "id": "9663e31d"
      },
      "outputs": [],
      "source": [
        " # TODO: Implement the random forest classifier here\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        " \n",
        "def randomforestfunction(training_data_x, training_data_y, test_data_x, test_data_y):\n",
        "  model = RandomForestClassifier(max_depth = 3, random_state = 0)\n",
        "  model.fit(training_data_x, training_data_y)\n",
        "  prediction = model.score(test_data_x, test_data_y)\n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e10363",
      "metadata": {
        "id": "65e10363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1df71145-bf21-4d09-c6f4-9c32819b86f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9730232558139534\n"
          ]
        }
      ],
      "source": [
        "# TODO: Perform some experiments here with different parameters of the random forest classifier. In the last line, print the accuracy with the best parameters.\n",
        "print(randomforestfunction(training_data_x, training_data_y, test_data_x, test_data_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a738769a",
      "metadata": {
        "id": "a738769a"
      },
      "source": [
        "TODO: Describe in one paragraph your experiments and evaluation of the random forest classifier. Consider things such as accuracy, training time, ease of tweaking of the parameters. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy seems to be less than the linear regression, but the training time and ease of tweaking of the parameters seem to be more efficient."
      ],
      "metadata": {
        "id": "GoMUnUUsFxG6"
      },
      "id": "GoMUnUUsFxG6"
    },
    {
      "cell_type": "markdown",
      "id": "4c46a9f4",
      "metadata": {
        "id": "4c46a9f4"
      },
      "source": [
        "## P6 Bonus: Implement an AdaBoost classifer using sklearn (5 pts)\n",
        "\n",
        "Implement an AdaBoost classifier using sklearn https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc373403",
      "metadata": {
        "id": "cc373403"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement the adaboost classifier here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "294dde6b",
      "metadata": {
        "id": "294dde6b"
      },
      "outputs": [],
      "source": [
        "# TODO: Perform some experiments here with different parametrizations of the adaboost classifier. In the last line, print the accuracy with the best parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d1dd74e",
      "metadata": {
        "id": "4d1dd74e"
      },
      "source": [
        "TODO: Describe in one paragraph your experiments and evaluation of the AdaBoost classifier. Consider things such as accuracy, training time, ease of tweaking of the parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cc7e2ea",
      "metadata": {
        "id": "6cc7e2ea"
      },
      "outputs": [],
      "source": [
        "# SOLUTION\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "clf = AdaBoostClassifier()\n",
        "clf.fit(training_data_x, training_data_y)\n",
        "yhat = clf.predict(test_data_x)\n",
        "acc = accuracy(test_data_y, yhat)\n",
        "print(f\"Accuracy of the AdaBoost classifier {acc}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "13140b3b9092b9a26a4b55ddc500d8b0c9f21b15e8ef2dd16bed19d6074a1e03"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}